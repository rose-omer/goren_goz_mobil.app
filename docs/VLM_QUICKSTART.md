# VLM Quick Start Guide

## ğŸš€ 5 Dakikada VLM Kurulumu

### Windows

```powershell
# 1. PowerShell'i yÃ¶netici olarak aÃ§
# 2. Script'i Ã§alÄ±ÅŸtÄ±r:
.\setup_vlm.ps1

# 3. Ollama server'Ä± baÅŸlat
ollama serve

# 4. Yeni terminal aÃ§Ä±p backend'i baÅŸlat
cd backend
python main.py

# 5. VLM'yi test et
python ../test_vlm.py
```

### macOS / Linux

```bash
# 1. Script'i Ã§alÄ±ÅŸtÄ±rÄ±labilir yap
chmod +x setup_vlm.sh

# 2. Script'i Ã§alÄ±ÅŸtÄ±r
./setup_vlm.sh

# 3. Ollama server'Ä± baÅŸlat
ollama serve

# 4. Yeni terminal aÃ§Ä±p backend'i baÅŸlat
cd backend
python main.py

# 5. VLM'yi test et
python ../test_vlm.py
```

---

## âœ¨ KullanÄ±labilecek Modeller

### SmolVLM (Ã–nerilen) âœ…
```bash
ollama pull smolvlm
# - Boyut: 500MB
# - HÄ±z: ~500ms/query
# - Kalite: Yeterli
```

### LLaVA 1.6 (7B - Daha DoÄŸru)
```bash
ollama pull llava:7b-v1.6
# - Boyut: 4GB
# - HÄ±z: ~1-2s/query
# - Kalite: YÃ¼ksek
```

### LLaVA 1.6 (13B - En DoÄŸru)
```bash
ollama pull llava:13b
# - Boyut: 7GB
# - HÄ±z: ~2-3s/query
# - Kalite: Ã‡ok yÃ¼ksek
```

---

## ğŸ§ª Test KomutlarÄ±

### Test 1: Server Durumu
```bash
# Ã‡alÄ±ÅŸan modelleri listele
ollama list
```

### Test 2: Simple Chat
```bash
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "smolvlm",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": false
  }'
```

### Test 3: Backend API
```bash
# Python test script'i Ã§alÄ±ÅŸtÄ±r
python test_vlm.py
```

### Test 4: GerÃ§ek Resim
```bash
# Resim yÃ¼kleyerek test et
curl -X POST http://localhost:8000/api/ask_context \
  -F "image=@your_photo.jpg" \
  -F "question=Bu resimde ne var?" \
  -F "use_cached_detections=false"
```

---

## ğŸ”§ Troubleshooting

### Ollama server Ã§alÄ±ÅŸmÄ±yor
```bash
# Ã‡alÄ±ÅŸan iÅŸlemi kontrol et
ps aux | grep ollama

# ManÃ¼el olarak baÅŸlat
ollama serve

# Port 11434'Ã¼ kontrol et
netstat -an | grep 11434
```

### Model indirimi yavaÅŸ
```bash
# Proxy kullan (gerekirse)
export HTTP_PROXY=...
export HTTPS_PROXY=...

# Daha kÃ¼Ã§Ã¼k model kullan
ollama pull smolvlm  # Yerine llava:7b-v1.6-q4
```

### Bellek sorunu
```bash
# CPU'da Ã§alÄ±ÅŸtÄ±r
export CUDA_VISIBLE_DEVICES=-1
ollama serve

# Daha kÃ¼Ã§Ã¼k model kullan
ollama pull smolvlm
```

### API timeouts
```bash
# config.yaml'da timeout'Ä± artÄ±r
vlm:
  timeout: 60  # 30'dan 60'a Ã§Ä±kart
```

---

## ğŸ“Š Performance Ä°puÃ§larÄ±

### Daha HÄ±zlÄ±
```bash
# 1. SmolVLM kullan
ollama pull smolvlm

# 2. Resim boyutunu kÃ¼Ã§Ã¼lt (config.yaml)
vlm:
  max_image_size: 384  # 512'den 384'e

# 3. Token limitini azalt
vlm:
  n_predict: 50  # 100'den 50'ye
```

### Daha DoÄŸru
```bash
# 1. Daha bÃ¼yÃ¼k model kullan
ollama pull llava:13b

# 2. Daha yÃ¼ksek temperature
vlm:
  temperature: 0.8  # 0.7'den 0.8'e
```

---

## ğŸ”Œ API Endpoints

### POST /api/ask_context
```bash
curl -X POST http://localhost:8000/api/ask_context \
  -F "image=@scene.jpg" \
  -F "question=Hangi taraftan tehlike var?" \
  -F "use_cached_detections=true"
```

**Response:**
```json
{
  "success": true,
  "answer": "SaÄŸ tarafta 2 metre uzakta bir araÃ§ var.",
  "processing_time_ms": 2500,
  "context_used": {
    "detections_count": 3,
    "cached": false
  },
  "metadata": {...}
}
```

### GET /api/preset_questions
```bash
curl http://localhost:8000/api/preset_questions
```

**Response:**
```json
{
  "success": true,
  "preset_questions": {
    "whats_ahead": "Ã–nÃ¼mde ne var?",
    "safe_to_cross": "KarÅŸÄ±ya geÃ§mek gÃ¼venli mi?",
    "nearest_obstacle": "En yakÄ±n engel nerede?",
    ...
  }
}
```

---

## ğŸ“ KonfigÃ¼rasyon (config.yaml)

```yaml
vlm:
  enabled: true
  server_url: "http://localhost:11434"
  timeout: 30
  max_retries: 2
  model_name: "smolvlm"
  n_predict: 100
  temperature: 0.7
  max_image_size: 512
```

---

## âœ… Kontrol Listesi

- [ ] Ollama yÃ¼klÃ¼
- [ ] SmolVLM model indirildi (`ollama list`)
- [ ] `ollama serve` Ã§alÄ±ÅŸÄ±yor
- [ ] Backend baÅŸlatÄ±ldÄ± (`python main.py`)
- [ ] Test script geÃ§ti (`python test_vlm.py`)
- [ ] API endpoint'i cevap veriyor (`test_vlm.py`)
- [ ] Mobile app'de `/api/ask_context` kullanÄ±labiliyor

---

## ğŸ†˜ HÄ±zlÄ± Ã‡Ã¶zÃ¼mler

| Problem | Ã‡Ã¶zÃ¼m |
|---------|-------|
| `ConnectionError: Cannot connect to server` | `ollama serve` Ã§alÄ±ÅŸtÄ±r |
| `"smolvlm not found"` | `ollama pull smolvlm` Ã§alÄ±ÅŸtÄ±r |
| `Timeout` | `timeout: 60` olarak artÄ±r |
| `Out of memory` | Daha kÃ¼Ã§Ã¼k model kullan veya CPU'da Ã§alÄ±ÅŸtÄ±r |
| `Slow response` | SmolVLM kullan, `n_predict` deÄŸerini azalt |

---

## ğŸ“š Kaynaklar

- **Ollama Docs**: https://github.com/ollama/ollama
- **SmolVLM**: https://huggingface.co/xtuner/SmolVLM-256M
- **LLaVA**: https://llava-vl.github.io
- **DetaylÄ± Setup Guide**: docs/VLM_SETUP.md

---

**Herhangi bir sorun yaÅŸarsan `test_vlm.py` Ã§alÄ±ÅŸtÄ±rarak detaylÄ± log gÃ¶rebilirsin!** ğŸ¯
