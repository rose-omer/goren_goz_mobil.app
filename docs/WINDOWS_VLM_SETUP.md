# VLM Kurulumu - Windows (llama.cpp + SmolVLM-500M)

## ğŸ¯ Genel BakÄ±ÅŸ

**llama-server** (llama.cpp) + **SmolVLM-500M** kombinasyonu CPU'da Ã§alÄ±ÅŸmak iÃ§in optimize edilmiÅŸ setup.

- **RAM KullanÄ±mÄ±**: ~800MB
- **Disk AlanÄ±**: ~1.5GB
- **CPU Usage**: Orta (25-40%)
- **Cevap HÄ±zÄ±**: 2-5 saniye (CPU'da)
- **GPU**: Gerekli deÄŸil âœ…

---

## AdÄ±m 1: llama-server Binary Ä°ndir âœ… TAMAMLANDI

Windows x64 CPU versiyonunu indir:
- âœ… **`llama-b7598-bin-win-cpu-x64.zip`** (21.4 MB)
- Ã‡Ä±kart â†’ `C:\llama-server\` klasÃ¶rÃ¼ne koy

Test:
```powershell
cd C:\llama-server
.\llama-server.exe --version
```

Ã‡Ä±ktÄ±:
```
version: 7598
built with Clang 19.1.5 for Windows x86_64 âœ…
```

---

## AdÄ±m 2: llama-server'Ä± BaÅŸlat (Terminal 1) âœ… Ã‡ALIÅIYOR

```powershell
cd C:\llama-server

.\llama-server.exe -hf ggml-org/SmolVLM-500M-Instruct-GGUF `
  --host 127.0.0.1 `
  --port 8080 `
  -ngl 0
```

Ã‡Ä±ktÄ±:
```
main: model loaded
main: server is listening on http://127.0.0.1:8080 âœ…
```

**Bu terminal AÃ‡IK KALSIN!**

---

## AdÄ±m 3: config/config.yaml GÃ¼ncelle

DosyayÄ± aÃ§: `c:\Users\admin\Desktop\goren_goz_mobil.app\config\config.yaml`

```yaml
vlm:
  server_url: "http://localhost:8080"
  timeout: 60
  model_name: "smolvlm-500m"
  enabled: true
```

---

## AdÄ±m 4: Backend'i BaÅŸlat (Terminal 2)

```powershell
cd c:\Users\admin\Desktop\goren_goz_mobil.app\backend
python main.py
```

Beklenen:
```
INFO: Uvicorn running on http://0.0.0.0:8000 âœ…
```

---

## AdÄ±m 5: VLM Test Et (Terminal 3)

```powershell
cd c:\Users\admin\Desktop\goren_goz_mobil.app
python test_vlm.py
```

Beklenen:
```
âœ… PASS - VLM Connection
âœ… PASS - Image Analysis  
âœ… PASS - With Detections
âœ… PASS - Preset Questions

Total: 4/4 tests passed âœ…
```

---

## âœ… Final Kontrol Listesi

- [x] llama-server.exe indir ve Ã§alÄ±ÅŸtÄ±r (Port 8080)
- [x] Terminal 1: llama-server Ã§alÄ±ÅŸÄ±yor âœ…
- [ ] config/config.yaml gÃ¼ncelle
- [ ] Terminal 2: python main.py (Port 8000)
- [ ] Terminal 3: python test_vlm.py (4/4 baÅŸarÄ±lÄ±)

---

## ğŸ“Š 3 Terminal KullanÄ±mÄ±

```powershell
# Terminal 1
cd C:\llama-server
.\llama-server.exe -hf ggml-org/SmolVLM-500M-Instruct-GGUF `
  --host 127.0.0.1 --port 8080 -ngl 0

# Terminal 2
cd c:\Users\admin\Desktop\goren_goz_mobil.app\backend
python main.py

# Terminal 3
cd c:\Users\admin\Desktop\goren_goz_mobil.app
python test_vlm.py
```

---

## ğŸ› Sorun Giderme

### Port 8080 zaten kullanÄ±mda
```powershell
netstat -an | findstr 8080
.\llama-server.exe -hf ggml-org/SmolVLM-500M-Instruct-GGUF --port 8081
# config.yaml'da da: server_url: "http://localhost:8081"
```

### API timeout
```yaml
vlm:
  timeout: 120
```

### Ã‡ok yavaÅŸ
```powershell
.\llama-server.exe -hf ggml-org/SmolVLM-256M-Instruct-GGUF --host 127.0.0.1 --port 8080 -ngl 0
```

---

## ğŸ“š Kaynaklar

- **llama.cpp**: https://github.com/ggerganov/llama.cpp/releases
- **SmolVLM-500M**: https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF

---

**Durum**: llama-server âœ… Ã‡ALIÅIYOR  
**Sonraki**: Terminal 2 + 3 baÅŸlat
